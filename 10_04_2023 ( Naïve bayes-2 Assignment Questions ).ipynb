{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f4d2707",
   "metadata": {},
   "source": [
    "# PW SKILLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768f3abd",
   "metadata": {},
   "source": [
    "## Assignment Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd48a079",
   "metadata": {},
   "source": [
    "### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef0a2fa",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use the conditional probability formula.\n",
    "\n",
    "Let \n",
    "�\n",
    "S be the event that an employee is a smoker, and \n",
    "�\n",
    "H be the event that an employee uses the health insurance plan.\n",
    "\n",
    "The probability of an employee using the health insurance plan is denoted as \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(H), and the probability of an employee being a smoker is denoted as \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(S).\n",
    "\n",
    "The conditional probability of an employee being a smoker given that he/she uses the health insurance plan is denoted as \n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "P(S∣H) and is calculated using the formula:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "∩\n",
    "�\n",
    ")\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(S∣H)= \n",
    "P(H)\n",
    "P(S∩H)\n",
    "​\n",
    " \n",
    "\n",
    "Given the information provided:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∩\n",
    "�\n",
    ")\n",
    "P(S∩H) (probability of an employee being a smoker and using the health insurance plan) is the product of the probability of using the plan (\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(H)) and the probability of being a smoker given the use of the plan (\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "P(S∣H)):\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∩\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "⋅\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "P(S∩H)=P(H)⋅P(S∣H)\n",
    "\n",
    "Let's calculate it:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∩\n",
    "�\n",
    ")\n",
    "=\n",
    "0.70\n",
    "×\n",
    "0.40\n",
    "=\n",
    "0.28\n",
    "P(S∩H)=0.70×0.40=0.28\n",
    "\n",
    "Now, we can use the conditional probability formula:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "∩\n",
    "�\n",
    ")\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "P(S∣H)= \n",
    "P(H)\n",
    "P(S∩H)\n",
    "​\n",
    " \n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "0.28\n",
    "0.70\n",
    "P(S∣H)= \n",
    "0.70\n",
    "0.28\n",
    "​\n",
    " \n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    ")\n",
    "=\n",
    "0.4\n",
    "P(S∣H)=0.4\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is \n",
    "0.4\n",
    "0.4 or \n",
    "40\n",
    "40."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df08f0",
   "metadata": {},
   "source": [
    "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557a469a",
   "metadata": {},
   "source": [
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of data they are designed to handle and the underlying assumptions about the distribution of the features.\n",
    "\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Type of Data: Bernoulli Naive Bayes is designed for binary data, where features are binary variables (0 or 1) indicating the presence or absence of a particular feature.\n",
    "Assumption: Assumes that features are binary and follows a Bernoulli distribution.\n",
    "Use Cases: Commonly used in text classification problems where the representation of documents is based on the presence or absence of words.\n",
    "Example: Classifying documents as spam or non-spam based on the presence or absence of certain keywords.\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Type of Data: Multinomial Naive Bayes is designed for count data, where features represent the occurrence count of a particular term or word in a document.\n",
    "Assumption: Assumes that features are counts and follow a multinomial distribution.\n",
    "Use Cases: Widely used in text classification problems where the representation of documents is based on word frequencies or term frequencies.\n",
    "Example: Classifying documents into categories based on the frequency of words appearing in the documents.\n",
    "Key Points of Comparison:\n",
    "\n",
    "Feature Representation:\n",
    "\n",
    "Bernoulli: Binary features (0 or 1).\n",
    "Multinomial: Count or frequency-based features.\n",
    "Distribution Assumption:\n",
    "\n",
    "Bernoulli: Assumes a Bernoulli distribution for features.\n",
    "Multinomial: Assumes a multinomial distribution for features.\n",
    "Use Cases:\n",
    "\n",
    "Bernoulli: Suitable for binary feature data.\n",
    "Multinomial: Suitable for count or frequency-based feature data.\n",
    "Text Classification:\n",
    "\n",
    "Bernoulli: Often used in document classification tasks with binary feature representations.\n",
    "Multinomial: Commonly used in document classification tasks with count or frequency-based feature representations.\n",
    "Example Applications:\n",
    "\n",
    "Bernoulli: Email spam classification, sentiment analysis.\n",
    "Multinomial: Document categorization, topic classification.\n",
    "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of the data and how features are represented. If the features are binary (presence/absence), Bernoulli Naive Bayes is suitable. If the features are counts or frequencies, Multinomial Naive Bayes is more appropriate.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8cf0a7",
   "metadata": {},
   "source": [
    "### Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10f7d24",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes, like other Naive Bayes classifiers, generally assumes that missing values are implicitly indicative of the absence of a feature. This assumption aligns with the underlying probabilistic model, which considers the presence or absence of features in a binary manner (0 or 1). However, it's important to note that handling missing values in the context of Bernoulli Naive Bayes is not explicitly addressed within the classifier itself.\n",
    "\n",
    "Here are common approaches to handling missing values when using Bernoulli Naive Bayes:\n",
    "\n",
    "Imputation with a Default Value:\n",
    "\n",
    "Replace missing values with a default value that represents the absence of a feature. For binary features in Bernoulli Naive Bayes, this could be 0.\n",
    "This approach assumes that missing values indicate the absence of the corresponding feature.\n",
    "Imputation with Feature Statistics:\n",
    "\n",
    "Replace missing values with statistics derived from the available data, such as the mean or mode of the feature.\n",
    "This approach assumes that the distribution of the feature among the observed instances provides a reasonable estimate for missing values.\n",
    "Separate Treatment for Missing Values:\n",
    "\n",
    "Treat missing values as a separate category during model training and prediction.\n",
    "During model training, the absence of the feature could be explicitly modeled. During prediction, the missing values are treated as a separate category, and their impact on class probabilities is considered accordingly.\n",
    "Deletion of Instances or Features:\n",
    "\n",
    "Exclude instances with missing values or features with missing values from the training set.\n",
    "While this reduces the amount of available data, it ensures that only complete instances or features are used for training.\n",
    "Advanced Imputation Techniques:\n",
    "\n",
    "Employ more advanced imputation techniques, such as machine learning-based imputation models, to estimate missing values based on the relationships between features.\n",
    "It's crucial to consider the nature of the missing values and the assumptions made during imputation, as different approaches may be more suitable depending on the specific characteristics of the dataset. Additionally, the impact of the chosen imputation method on the overall model performance should be assessed through proper evaluation techniques, such as cross-validation.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8925339f",
   "metadata": {},
   "source": [
    "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d63f34",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that assumes the features within each class are normally distributed. While it is commonly associated with binary and continuous features, it can be adapted for multi-class classification problems.\n",
    "\n",
    "In the context of multi-class classification:\n",
    "\n",
    "Gaussian Naive Bayes for Binary Classification:\n",
    "\n",
    "For binary classification problems, Gaussian Naive Bayes models the distribution of features for each class as a Gaussian distribution.\n",
    "The decision rule involves comparing the likelihoods of the data under each class, considering the Gaussian distribution parameters for each class.\n",
    "Extension to Multi-Class Classification:\n",
    "\n",
    "Gaussian Naive Bayes can be extended to handle multiple classes by applying the same principles. Each class is associated with a Gaussian distribution for each feature.\n",
    "The decision rule is then based on comparing the likelihoods of the data under each class, considering the Gaussian distribution parameters for each class.\n",
    "One-vs-All (OvA) or One-vs-One (OvO) Strategy:\n",
    "\n",
    "For multi-class problems, Gaussian Naive Bayes can use a one-vs-all (OvA) or one-vs-one (OvO) strategy.\n",
    "In the OvA strategy, a separate Gaussian Naive Bayes model is trained for each class, and the class with the highest posterior probability is predicted.\n",
    "In the OvO strategy, a separate binary classifier is trained for each pair of classes, and a voting mechanism is used to determine the final class.\n",
    "Scikit-Learn Implementation:\n",
    "\n",
    "Scikit-learn, a popular machine learning library in Python, includes a GaussianNB class that can be used for multi-class classification.\n",
    "When applying it to a multi-class problem, the algorithm internally handles the OvA or OvO strategy, depending on the specific function or method used.\n",
    "Here's a simple example of using Gaussian Naive Bayes for multi-class classification in Python with Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset and split it into features (X) and target (y)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the classifier\n",
    "# gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "# y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2ea6f",
   "metadata": {},
   "source": [
    "In this example, the GaussianNB class is used for multi-class classification, and the OvA or OvO strategy is internally handled by the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422d179",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "Data preparation:\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message \n",
    "is spam or not based on several input features.\n",
    "\n",
    "\n",
    "Implementation:\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the \n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the \n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "\n",
    "Results:\n",
    "\n",
    "Report the following performance metrics for each classifier:\n",
    "\n",
    "Accuracy\n",
    "\n",
    "Precision\n",
    "\n",
    "Recall\n",
    "\n",
    "F1 score\n",
    "\n",
    "\n",
    "Discussion:\n",
    "\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is \n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a416bf",
   "metadata": {},
   "source": [
    "1. Data Preparation:\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository.\n",
    "Load the dataset into a Pandas DataFrame.\n",
    "Split the data into features (X) and the target variable (y).\n",
    "2. Implementation:\n",
    "\n",
    "Import the necessary libraries from scikit-learn: from sklearn.model_selection import cross_val_score, KFold, from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB.\n",
    "Create instances of each Naive Bayes classifier: BernoulliNB(), MultinomialNB(), GaussianNB().\n",
    "Use 10-fold cross-validation to evaluate the performance of each classifier: cross_val_score(model, X, y, cv=10, scoring='accuracy').\n",
    "3. Results:\n",
    "\n",
    "Report the accuracy, precision, recall, and F1 score for each classifier. You can use cross_val_score with different scoring metrics.\n",
    "4. Discussion:\n",
    "\n",
    "Discuss the obtained results. Which variant of Naive Bayes performed the best in terms of accuracy, precision, recall, and F1 score?\n",
    "Consider any insights into the strengths or limitations of each variant that you observed.\n",
    "5. Conclusion:\n",
    "\n",
    "Summarize your findings, mentioning which Naive Bayes variant performed the best.\n",
    "Provide suggestions for future work, such as exploring hyperparameter tuning or considering other classification algorithms.\n",
    "Here's a simple example code snippet to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa0bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "columns = []  # Add column names based on the dataset description\n",
    "data = pd.read_csv(url, header=None, names=columns)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "# Create instances of each Naive Bayes classifier\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Use 10-fold cross-validation to evaluate each classifier\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to calculate various metrics\n",
    "def calculate_metrics(model, X, y):\n",
    "    accuracy = cross_val_score(model, X, y, cv=kf, scoring='accuracy').mean()\n",
    "    precision = cross_val_score(model, X, y, cv=kf, scoring='precision').mean()\n",
    "    recall = cross_val_score(model, X, y, cv=kf, scoring='recall').mean()\n",
    "    f1 = cross_val_score(model, X, y, cv=kf, scoring='f1').mean()\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Calculate metrics for each classifier\n",
    "accuracy_bernoulli, precision_bernoulli, recall_bernoulli, f1_bernoulli = calculate_metrics(bernoulli_nb, X, y)\n",
    "accuracy_multinomial, precision_multinomial, recall_multinomial, f1_multinomial = calculate_metrics(multinomial_nb, X, y)\n",
    "accuracy_gaussian, precision_gaussian, recall_gaussian, f1_gaussian = calculate_metrics(gaussian_nb, X, y)\n",
    "\n",
    "# Report the results\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_bernoulli)\n",
    "print(\"Precision:\", precision_bernoulli)\n",
    "print(\"Recall:\", recall_bernoulli)\n",
    "print(\"F1 Score:\", f1_bernoulli)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_multinomial)\n",
    "print(\"Precision:\", precision_multinomial)\n",
    "print(\"Recall:\", recall_multinomial)\n",
    "print(\"F1 Score:\", f1_multinomial)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_gaussian)\n",
    "print(\"Precision:\", precision_gaussian)\n",
    "print(\"Recall:\", recall_gaussian)\n",
    "print(\"F1 Score:\", f1_gaussian)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91744c8",
   "metadata": {},
   "source": [
    "Remember to replace columns with the actual column names based on the dataset description. Additionally, ensure that the dataset is loaded correctly, and adjust any other details according to the specific characteristics of the Spambase dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb1699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f0c15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
